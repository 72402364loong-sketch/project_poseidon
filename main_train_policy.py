#!/usr/bin/env python3
"""
Stage 2: Dynamic Policy Learning with DAgger
é˜¶æ®µ2ï¼šåŸºäºDAggerçš„åŠ¨æ€ç­–ç•¥å­¦ä¹ ä¸»è®­ç»ƒè„šæœ¬
"""

import os
import sys
import argparse
import yaml
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import numpy as np
import random
import logging
from datetime import datetime
import json
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from models.policy_model import PolicyModel, DAggerTrainer
from models.representation_model import RepresentationModel
from models.classifier import ObjectClassifier
from data_loader.dataset import PolicyDataset
from data_loader.samplers import DAggerSampler
from engine.trainer import train_policy_epoch
from engine.evaluator import evaluate_policy_epoch, get_action_with_uncertainty, should_request_expert_annotation
from engine.losses import PolicyLoss
from robot.interface import RobotInterface, VisionSensor, TactileSensor
from robot.expert import ExpertFactory


def setup_logging(log_dir: str) -> logging.Logger:
    """è®¾ç½®æ—¥å¿—"""
    os.makedirs(log_dir, exist_ok=True)
    
    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"stage2_policy_{timestamp}.log")
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    return logging.getLogger(__name__)


def set_seed(seed: int) -> None:
    """è®¾ç½®éšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def load_config(config_path: str) -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def create_representation_model(config: dict, device: torch.device) -> RepresentationModel:
    """åˆ›å»ºå¹¶åŠ è½½è¡¨å¾æ¨¡å‹"""
    model_config = config['model_params']
    rep_config = model_config['representation_model']
    
    # åˆ›å»ºè¡¨å¾æ¨¡å‹ï¼ˆçº¯CLIPå˜ä½“ï¼‰
    rep_model = RepresentationModel(
        vision_encoder_weights_path=None,  # ç¨åä»æ£€æŸ¥ç‚¹åŠ è½½
        embed_dim=128  # ä½¿ç”¨é»˜è®¤å€¼ï¼Œç¨åä»æ£€æŸ¥ç‚¹åŠ è½½
    )
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    weights_path = rep_config['weights_path']
    if os.path.exists(weights_path):
        checkpoint = torch.load(weights_path, map_location=device)
        if 'model_state_dict' in checkpoint:
            rep_model.load_state_dict(checkpoint['model_state_dict'])
        else:
            rep_model.load_state_dict(checkpoint)
        print(f"Loaded representation model from: {weights_path}")
    else:
        print(f"Warning: Representation model weights not found at {weights_path}")
    
    # å†»ç»“è¡¨å¾æ¨¡å‹
    if rep_config.get('freeze', True):
        rep_model.freeze_encoders()
    
    return rep_model.to(device)


def create_classifier_model(config: dict, device: torch.device) -> ObjectClassifier:
    """åˆ›å»ºå¹¶åŠ è½½åˆ†ç±»å™¨æ¨¡å‹"""
    model_config = config['model_params']
    classifier_config = model_config.get('classifier_model', {})
    
    # åˆ›å»ºåˆ†ç±»å™¨
    classifier = ObjectClassifier(
        feature_dim=classifier_config.get('feature_dim', 1536),  # 768 + 768
        hidden_dim=classifier_config.get('hidden_dim', 512),
        num_classes=classifier_config.get('num_classes', 15),
        dropout=classifier_config.get('dropout', 0.2)
    )
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    weights_path = classifier_config.get('weights_path')
    if weights_path and os.path.exists(weights_path):
        checkpoint = torch.load(weights_path, map_location=device)
        if 'classifier_state_dict' in checkpoint:
            classifier.load_state_dict(checkpoint['classifier_state_dict'])
        else:
            classifier.load_state_dict(checkpoint)
        print(f"Loaded classifier from: {weights_path}")
    else:
        print(f"Warning: Classifier weights not found at {weights_path}")
    
    # å†»ç»“åˆ†ç±»å™¨
    classifier.eval()
    for param in classifier.parameters():
        param.requires_grad = False
    
    return classifier.to(device)


def create_policy_model(config: dict, device: torch.device) -> PolicyModel:
    """åˆ›å»ºç­–ç•¥æ¨¡å‹"""
    model_config = config['model_params']['policy_model']
    classifier_config = config['model_params'].get('classifier_model', {})
    
    policy_model = PolicyModel(
        vision_feature_dim=model_config.get('vision_feature_dim', 768),
        tactile_feature_dim=model_config.get('tactile_feature_dim', 768),
        geometry_feature_dim=model_config.get('geometry_feature_dim', 3),
        classification_feature_dim=classifier_config.get('num_classes', 15),  # æ–°å¢åˆ†ç±»ç‰¹å¾ç»´åº¦
        lstm_hidden_dim=model_config['lstm_hidden_dim'],
        lstm_num_layers=model_config['lstm_num_layers'],
        lstm_dropout=model_config['lstm_dropout'],
        action_dim=model_config['action_dim'],
        mlp_hidden_dims=model_config.get('mlp_hidden_dims', [256, 128]),
        mlp_dropout=model_config['mlp_dropout']
    )
    
    policy_model = policy_model.to(device)
    policy_model.print_model_info()
    
    return policy_model


def create_datasets(config: dict) -> tuple:
    """åˆ›å»ºæ•°æ®é›†"""
    data_config = config['data_params']
    
    # åˆ›å»ºåˆå§‹ä¸“å®¶æ•°æ®é›†
    train_dataset = PolicyDataset(
        data_path=data_config['dataset_path'],
        split='train',
        trajectory_length=data_config['trajectory_length'],
        tactile_seq_len=100,  # ä¸è¡¨å¾æ¨¡å‹ä¿æŒä¸€è‡´
        include_geometry=True
    )
    
    val_dataset = PolicyDataset(
        data_path=data_config['dataset_path'],
        split='val',
        trajectory_length=data_config['trajectory_length'],
        tactile_seq_len=100,
        include_geometry=True
    )
    
    return train_dataset, val_dataset


def create_data_loaders(train_dataset, val_dataset, config: dict, expert_indices: List[int] = None, policy_indices: List[int] = None) -> tuple:
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    data_config = config['data_params']
    dagger_config = config.get('dagger_params', {})
    
    # å¦‚æœæä¾›äº†DAggeré‡‡æ ·ç´¢å¼•ï¼Œä½¿ç”¨DAggeré‡‡æ ·å™¨
    if expert_indices is not None and policy_indices is not None:
        train_sampler = DAggerSampler(
            expert_indices=expert_indices,
            policy_indices=policy_indices,
            batch_size=data_config['batch_size'],
            expert_ratio=dagger_config.get('expert_data_ratio', 0.5),
            shuffle=True,
            drop_last=True
        )
        
        train_loader = DataLoader(
            train_dataset,
            batch_sampler=train_sampler,
            num_workers=data_config['num_workers'],
            pin_memory=data_config.get('pin_memory', True)
        )
    else:
        # å¸¸è§„æ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset,
            batch_size=data_config['batch_size'],
            shuffle=True,
            num_workers=data_config['num_workers'],
            pin_memory=data_config.get('pin_memory', True),
            drop_last=True
        )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=data_config['batch_size'],
        shuffle=False,
        num_workers=data_config['num_workers'],
        pin_memory=data_config.get('pin_memory', True),
        drop_last=False
    )
    
    return train_loader, val_loader


def create_optimizer_and_scheduler(model: nn.Module, config: dict) -> tuple:
    """åˆ›å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
    train_config = config['training_params']
    scheduler_config = config.get('scheduler_params', {})
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = optim.Adam(
        model.parameters(),
        lr=train_config['learning_rate'],
        weight_decay=train_config['weight_decay']
    )
    
    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = None
    if scheduler_config.get('type') == 'step':
        scheduler = optim.lr_scheduler.StepLR(
            optimizer,
            step_size=scheduler_config.get('step_size', 20),
            gamma=scheduler_config.get('gamma', 0.8)
        )
    
    return optimizer, scheduler


def create_loss_function(config: dict) -> nn.Module:
    """åˆ›å»ºæŸå¤±å‡½æ•°"""
    loss_config = config['loss_params']
    
    loss_fn = PolicyLoss(
        action_loss_type=loss_config.get('type', 'mse'),
        action_weight=loss_config.get('action_weight', 1.0),
        regularization_weight=loss_config.get('regularization_weight', 0.01),
        velocity_weight=0.1,
        smoothness_weight=0.05
    )
    
    return loss_fn


def create_robot_interface(config: dict) -> RobotInterface:
    """åˆ›å»ºæœºå™¨äººæ¥å£"""
    robot_config = config.get('robot_params', {})
    
    # åˆ›å»ºæœºå™¨äººæ¥å£
    robot_interface = RobotInterface(robot_config)
    
    # æ·»åŠ ä¼ æ„Ÿå™¨
    vision_sensor = VisionSensor('stereo_camera', frequency=30.0)
    tactile_sensor = TactileSensor('tactile_array', num_sensors=18, frequency=1000.0)
    
    robot_interface.add_sensor(vision_sensor)
    robot_interface.add_sensor(tactile_sensor)
    
    return robot_interface


def expert_policy_function(state: np.ndarray) -> np.ndarray:
    """
    ä¸“å®¶ç­–ç•¥å‡½æ•°ï¼ˆå ä½ç¬¦å®ç°ï¼‰
    åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™åº”è¯¥æ˜¯äººç±»ä¸“å®¶æˆ–é«˜çº§æ§åˆ¶å™¨æä¾›çš„ç­–ç•¥
    """
    # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„ä¸“å®¶ç­–ç•¥ç¤ºä¾‹
    # å®é™…åº”ç”¨ä¸­éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡å®ç°
    
    # æå–çŠ¶æ€ä¿¡æ¯
    # stateåŒ…å«: [vision_features(768), tactile_features(768), geometry_features(3)]
    
    # ç®€å•çš„æ¯”ä¾‹æ§åˆ¶ç¤ºä¾‹
    if len(state) >= 3:
        # å‡è®¾æœ€å3ä¸ªå…ƒç´ æ˜¯3Dåæ ‡
        target_pos = state[-3:]
        # ç®€å•çš„ä½ç½®æ§åˆ¶
        action = np.clip(target_pos * 0.1, -0.1, 0.1)
        # è¡¥å……åˆ°6DOF
        if len(action) < 6:
            action = np.pad(action, (0, 6 - len(action)), 'constant')
    else:
        action = np.zeros(6)
    
    return action


def collect_policy_rollouts(
    policy_model: PolicyModel,
    robot_interface: RobotInterface,
    representation_model: RepresentationModel,
    classifier: ObjectClassifier,
    expert: Any,
    config: dict,
    num_episodes: int = 10
) -> List[Dict[str, Any]]:
    """
    æ”¶é›†ç­–ç•¥æ‰§è¡Œçš„è½¨è¿¹æ•°æ®ï¼ˆé›†æˆä¸»åŠ¨å­¦ä¹ ï¼‰
    
    Args:
        policy_model: ç­–ç•¥æ¨¡å‹
        robot_interface: æœºå™¨äººæ¥å£
        representation_model: è¡¨å¾æ¨¡å‹
        classifier: åˆ†ç±»å™¨æ¨¡å‹
        expert: ä¸“å®¶æ¥å£
        config: é…ç½®
        num_episodes: episodeæ•°é‡
    
    Returns:
        è½¨è¿¹æ•°æ®åˆ—è¡¨
    """
    policy_model.eval()
    representation_model.eval()
    
    rollouts = []
    device = next(policy_model.parameters()).device
    
    dagger_config = config.get('dagger_params', {})
    max_episode_length = dagger_config.get('max_episode_length', 100)
    
    # --- æ–°å¢/ä¿®æ”¹ Start ---
    # è·å–ä¸»åŠ¨å­¦ä¹ é…ç½®
    active_learning_config = config.get('active_learning_params', {})
    active_learning_enabled = active_learning_config.get('enabled', False)
    mc_samples = active_learning_config.get('mc_dropout_samples', 25)
    arm_threshold = active_learning_config.get('arm_uncertainty_threshold', 0.1)
    gripper_threshold = active_learning_config.get('gripper_uncertainty_threshold', 0.05)
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_steps = 0
    expert_requests = 0
    # --- æ–°å¢/ä¿®æ”¹ End ---
    
    with torch.no_grad():
        for episode in range(num_episodes):
            print(f"Collecting episode {episode + 1}/{num_episodes}")
            
            # åˆå§‹åŒ–episode
            states = []
            actions = []
            expert_actions = []  # å­˜å‚¨ä¸“å®¶åŠ¨ä½œ
            uncertainty_scores = []  # å­˜å‚¨ä¸ç¡®å®šæ€§åˆ†æ•°
            
            # åˆå§‹åŒ–LSTMéšè—çŠ¶æ€
            hidden_state = policy_model.init_hidden_state(1, device)
            
            for step in range(max_episode_length):
                # è·å–åŒæ­¥çš„ä¼ æ„Ÿå™¨æ•°æ®
                sensor_data = robot_interface.get_synchronized_sensor_data()
                
                if sensor_data is None:
                    continue
                
                # æå–è§†è§‰å’Œè§¦è§‰æ•°æ®
                vision_data = sensor_data.get('stereo_camera')
                tactile_data = sensor_data.get('tactile_array')
                
                if vision_data is None or tactile_data is None:
                    continue
                
                # è½¬æ¢æ•°æ®æ ¼å¼
                vision_tensor = torch.from_numpy(vision_data.data).float().unsqueeze(0).to(device)
                tactile_tensor = torch.from_numpy(tactile_data.data).float().unsqueeze(0).unsqueeze(0).to(device)
                
                # é€šè¿‡è¡¨å¾æ¨¡å‹æå–ç‰¹å¾
                vision_features, tactile_features = representation_model(vision_tensor, tactile_tensor)
                
                # é€šè¿‡åˆ†ç±»å™¨è·å–åˆ†ç±»ç‰¹å¾
                with torch.no_grad():
                    combined_features = torch.cat([vision_features, tactile_features], dim=1)
                    classification_logits = classifier(combined_features)
                
                # è®¡ç®—å‡ ä½•ç‰¹å¾ï¼ˆ3Dåæ ‡ï¼‰
                # è¿™é‡Œéœ€è¦å®ç°å…·ä½“çš„å‡ ä½•è®¡ç®—é€»è¾‘
                geometry_features = torch.zeros(1, 3).to(device)  # å ä½ç¬¦
                
                # æ„å»ºçŠ¶æ€å‘é‡ï¼ˆåŒ…å«åˆ†ç±»ç‰¹å¾ï¼‰
                state_vector = torch.cat([
                    vision_features,
                    tactile_features,
                    geometry_features,
                    classification_logits
                ], dim=1)
                
                states.append(state_vector.cpu().numpy())
                
                # --- æ–°å¢/ä¿®æ”¹ Start ---
                # ä¸»åŠ¨å­¦ä¹ ï¼šä½¿ç”¨MC Dropoutè®¡ç®—ä¸ç¡®å®šæ€§å’ŒåŠ¨ä½œ
                if active_learning_enabled:
                    # ä½¿ç”¨MC Dropoutè·å–åŠ¨ä½œå’Œä¸ç¡®å®šæ€§
                    robot_action, arm_uncertainty, gripper_uncertainty = get_action_with_uncertainty(
                        policy_model,
                        state_vector,
                        hidden_state,
                        mc_samples
                    )
                    
                    # åˆ¤æ–­æ˜¯å¦éœ€è¦ä¸“å®¶æ ‡æ³¨
                    need_expert = should_request_expert_annotation(
                        arm_uncertainty, gripper_uncertainty, arm_threshold, gripper_threshold
                    )
                    
                    if need_expert:
                        # è¯·æ±‚ä¸“å®¶æ ‡æ³¨
                        print(f"ğŸ¤– é«˜ä¸ç¡®å®šæ€§! Arm: {arm_uncertainty:.4f}, Gripper: {gripper_uncertainty:.4f}. è¯·æ±‚ä¸“å®¶æ ‡æ³¨...")
                        
                        # æ„å»ºå½“å‰çŠ¶æ€å­—å…¸ä¾›ä¸“å®¶ä½¿ç”¨
                        current_state = {
                            'position': geometry_features.cpu().numpy().flatten().tolist(),
                            'vision_features': vision_features.cpu().numpy().flatten().tolist(),
                            'tactile_features': tactile_features.cpu().numpy().flatten().tolist(),
                            'classification_logits': classification_logits.cpu().numpy().flatten().tolist()
                        }
                        
                        expert_action = expert.get_label(current_state)
                        expert_actions.append(expert_action.cpu().numpy())
                        expert_requests += 1
                        
                        # ä½¿ç”¨ä¸“å®¶åŠ¨ä½œè¿›è¡Œè®­ç»ƒï¼Œä½†æ‰§è¡Œæœºå™¨äººåŠ¨ä½œ
                        final_action = robot_action
                    else:
                        # ä¸ç¡®å®šæ€§è¾ƒä½ï¼Œæœºå™¨äººè‡ªä¸»å†³ç­–
                        expert_actions.append(None)  # æ ‡è®°ä¸ºæ— ä¸“å®¶æ ‡æ³¨
                        final_action = robot_action
                    
                    # è®°å½•ä¸ç¡®å®šæ€§åˆ†æ•°
                    uncertainty_scores.append({
                        'arm_uncertainty': arm_uncertainty,
                        'gripper_uncertainty': gripper_uncertainty,
                        'total_uncertainty': arm_uncertainty + gripper_uncertainty
                    })
                    
                    # æ›´æ–°éšè—çŠ¶æ€ï¼ˆä½¿ç”¨æœºå™¨äººåŠ¨ä½œï¼‰
                    _, hidden_state = policy_model.predict_step(state_vector, hidden_state)
                    
                else:
                    # ä¼ ç»ŸDAggerï¼šæ¯æ¬¡éƒ½è¯·æ±‚ä¸“å®¶æ ‡æ³¨
                    predicted_action, hidden_state = policy_model.predict_step(
                        state_vector, hidden_state
                    )
                    
                    # æ„å»ºå½“å‰çŠ¶æ€å­—å…¸ä¾›ä¸“å®¶ä½¿ç”¨
                    current_state = {
                        'position': geometry_features.cpu().numpy().flatten().tolist(),
                        'vision_features': vision_features.cpu().numpy().flatten().tolist(),
                        'tactile_features': tactile_features.cpu().numpy().flatten().tolist(),
                        'classification_logits': classification_logits.cpu().numpy().flatten().tolist()
                    }
                    
                    expert_action = expert.get_label(current_state)
                    expert_actions.append(expert_action.cpu().numpy())
                    expert_requests += 1
                    
                    final_action = predicted_action
                    uncertainty_scores.append(None)  # ä¼ ç»Ÿæ¨¡å¼ä¸‹æ— ä¸ç¡®å®šæ€§åˆ†æ•°
                # --- æ–°å¢/ä¿®æ”¹ End ---
                
                # åº”ç”¨åŠ¨ä½œçº¦æŸ
                action_constraints = {
                    'velocity_limit': config.get('robot_params', {}).get('velocity_limit', 0.1)
                }
                constrained_action = policy_model.apply_action_constraints(
                    final_action, action_constraints
                )
                
                actions.append(constrained_action.cpu().numpy())
                total_steps += 1
                
                # æ‰§è¡ŒåŠ¨ä½œï¼ˆåœ¨å®é™…æœºå™¨äººä¸Šï¼‰
                # è¿™é‡Œéœ€è¦å°†åŠ¨ä½œå‘é€ç»™æœºå™¨äºº
                
                # æ£€æŸ¥ä»»åŠ¡å®Œæˆæ¡ä»¶
                # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡å®ç°
                
            rollouts.append({
                'episode_id': episode,
                'states': states,
                'actions': actions,
                'expert_actions': expert_actions,  # æ–°å¢ï¼šä¸“å®¶åŠ¨ä½œ
                'uncertainty_scores': uncertainty_scores,  # æ–°å¢ï¼šä¸ç¡®å®šæ€§åˆ†æ•°
                'length': len(states)
            })
    
    # --- æ–°å¢/ä¿®æ”¹ Start ---
    # æ‰“å°ä¸»åŠ¨å­¦ä¹ ç»Ÿè®¡ä¿¡æ¯
    if active_learning_enabled:
        expert_request_rate = expert_requests / max(total_steps, 1) * 100
        print(f"\nğŸ“Š ä¸»åŠ¨å­¦ä¹ ç»Ÿè®¡:")
        print(f"   æ€»æ­¥æ•°: {total_steps}")
        print(f"   ä¸“å®¶è¯·æ±‚æ¬¡æ•°: {expert_requests}")
        print(f"   ä¸“å®¶è¯·æ±‚ç‡: {expert_request_rate:.2f}%")
        print(f"   èŠ‚çœæ ‡æ³¨: {total_steps - expert_requests} æ­¥")
    # --- æ–°å¢/ä¿®æ”¹ End ---
    
    return rollouts


def run_dagger_iteration(
    policy_model: PolicyModel,
    representation_model: RepresentationModel,
    classifier: ObjectClassifier,
    expert: Any,
    robot_interface: RobotInterface,
    config: dict,
    iteration: int,
    logger: logging.Logger
) -> List[Dict[str, Any]]:
    """
    è¿è¡Œä¸€æ¬¡DAggerè¿­ä»£ï¼ˆé›†æˆä¸»åŠ¨å­¦ä¹ ï¼‰
    
    Args:
        policy_model: ç­–ç•¥æ¨¡å‹
        representation_model: è¡¨å¾æ¨¡å‹
        classifier: åˆ†ç±»å™¨æ¨¡å‹
        expert: ä¸“å®¶æ¥å£
        robot_interface: æœºå™¨äººæ¥å£
        config: é…ç½®
        iteration: è¿­ä»£æ¬¡æ•°
        logger: æ—¥å¿—è®°å½•å™¨
    
    Returns:
        æ–°æ”¶é›†çš„è®­ç»ƒæ•°æ®
    """
    dagger_config = config.get('dagger_params', {})
    episodes_per_iteration = dagger_config.get('episodes_per_iteration', 50)
    
    logger.info(f"Starting DAgger iteration {iteration}")
    
    # 1. ä½¿ç”¨å½“å‰ç­–ç•¥æ”¶é›†è½¨è¿¹ï¼ˆé›†æˆä¸»åŠ¨å­¦ä¹ ï¼‰
    logger.info("Collecting policy rollouts...")
    rollouts = collect_policy_rollouts(
        policy_model, robot_interface, representation_model, classifier, expert,
        config, episodes_per_iteration
    )
    
    # 2. ä¸“å®¶æ ‡æ³¨ï¼ˆåœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œéœ€è¦äººç±»ä¸“å®¶ä»‹å…¥ï¼‰
    logger.info("Collecting expert corrections...")
    corrected_data = []
    
    for rollout in rollouts:
        states = rollout['states']
        actions = rollout['actions']
        
        # ä¸ºæ¯ä¸ªçŠ¶æ€è·å–ä¸“å®¶åŠ¨ä½œ
        expert_actions = []
        for state in states:
            expert_action = expert_policy_function(state.flatten())
            expert_actions.append(expert_action)
        
        corrected_data.append({
            'states': states,
            'actions': expert_actions,
            'episode_id': rollout['episode_id'],
            'iteration': iteration
        })
    
    logger.info(f"Collected {len(corrected_data)} corrected trajectories")
    
    return corrected_data


def save_checkpoint(
    policy_model: PolicyModel,
    optimizer: optim.Optimizer,
    scheduler,
    epoch: int,
    iteration: int,
    best_success_rate: float,
    checkpoint_dir: str,
    is_best: bool = False
) -> None:
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    checkpoint = {
        'epoch': epoch,
        'iteration': iteration,
        'model_state_dict': policy_model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'best_success_rate': best_success_rate,
    }
    
    if scheduler:
        checkpoint['scheduler_state_dict'] = scheduler.state_dict()
    
    # ä¿å­˜å¸¸è§„æ£€æŸ¥ç‚¹
    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_iter_{iteration}_epoch_{epoch}.pth')
    torch.save(checkpoint, checkpoint_path)
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if is_best:
        best_path = os.path.join(checkpoint_dir, 'best_policy_model.pth')
        torch.save(checkpoint, best_path)
        print(f"Best policy model saved with success rate: {best_success_rate:.4f}")


def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='Stage 2: Dynamic Policy Learning with DAgger')
    parser.add_argument('--config', type=str, required=True,
                       help='Path to configuration file')
    parser.add_argument('--resume', type=str, default=None,
                       help='Path to checkpoint to resume from')
    parser.add_argument('--seed', type=int, default=42,
                       help='Random seed')
    parser.add_argument('--skip_robot', action='store_true',
                       help='Skip robot interface (for testing)')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = load_config(args.config)
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() and config['device_params']['use_cuda'] else 'cpu')
    print(f"Using device: {device}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    checkpoint_dir = config['checkpoint_params']['output_dir']
    log_dir = config['logging_params']['tensorboard_dir']
    os.makedirs(checkpoint_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging(log_dir)
    logger.info(f"Starting Stage 2 training with config: {args.config}")
    
    # åˆ›å»ºè¡¨å¾æ¨¡å‹ï¼ˆå†»ç»“ï¼‰
    logger.info("Loading representation model...")
    representation_model = create_representation_model(config, device)
    representation_model.eval()
    
    # åˆ›å»ºåˆ†ç±»å™¨æ¨¡å‹ï¼ˆå†»ç»“ï¼‰
    logger.info("Loading classifier model...")
    classifier = create_classifier_model(config, device)
    
    # --- æ–°å¢/ä¿®æ”¹ Start ---
    # åˆ›å»ºä¸“å®¶æ¥å£
    logger.info("Creating expert interface...")
    expert = ExpertFactory.create_expert(config)
    # --- æ–°å¢/ä¿®æ”¹ End ---
    
    # åˆ›å»ºç­–ç•¥æ¨¡å‹
    logger.info("Creating policy model...")
    policy_model = create_policy_model(config, device)
    
    # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer, scheduler = create_optimizer_and_scheduler(policy_model, config)
    
    # åˆ›å»ºæŸå¤±å‡½æ•°
    loss_fn = create_loss_function(config)
    
    # åˆ›å»ºæœºå™¨äººæ¥å£ï¼ˆå¦‚æœä¸è·³è¿‡ï¼‰
    robot_interface = None
    if not args.skip_robot:
        logger.info("Creating robot interface...")
        robot_interface = create_robot_interface(config)
        robot_interface.connect()
    
    # åˆ›å»ºåˆå§‹æ•°æ®é›†
    logger.info("Creating initial datasets...")
    train_dataset, val_dataset = create_datasets(config)
    
    logger.info(f"Initial train dataset size: {len(train_dataset)}")
    logger.info(f"Validation dataset size: {len(val_dataset)}")
    
    # åˆ›å»ºTensorBoardå†™å…¥å™¨
    writer = SummaryWriter(log_dir)
    
    # DAggerå‚æ•°
    dagger_config = config.get('dagger_params', {})
    max_iterations = dagger_config.get('max_iterations', 10)
    initial_epochs = dagger_config.get('initial_epochs', 50)
    iteration_epochs = dagger_config.get('iteration_epochs', 30)
    
    # æ¢å¤è®­ç»ƒï¼ˆå¦‚æœæŒ‡å®šï¼‰
    start_iteration = 0
    start_epoch = 1
    best_success_rate = 0.0
    aggregated_data = []
    
    if args.resume:
        logger.info(f"Resuming from checkpoint: {args.resume}")
        checkpoint = torch.load(args.resume, map_location=device)
        policy_model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        if scheduler and 'scheduler_state_dict' in checkpoint:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        start_iteration = checkpoint.get('iteration', 0)
        start_epoch = checkpoint['epoch'] + 1
        best_success_rate = checkpoint.get('best_success_rate', 0.0)
    
    # DAggerè¿­ä»£å¾ªç¯
    for iteration in range(start_iteration, max_iterations):
        logger.info(f"\n=== DAgger Iteration {iteration + 1}/{max_iterations} ===")
        
        # ç¡®å®šå½“å‰è¿­ä»£çš„è®­ç»ƒepochæ•°
        current_epochs = initial_epochs if iteration == 0 else iteration_epochs
        
        # å¦‚æœä¸æ˜¯ç¬¬ä¸€æ¬¡è¿­ä»£ä¸”æœ‰æœºå™¨äººæ¥å£ï¼Œæ”¶é›†æ–°æ•°æ®
        if iteration > 0 and robot_interface is not None:
            new_data = run_dagger_iteration(
                policy_model, representation_model, classifier, expert, robot_interface,
                config, iteration, logger
            )
            aggregated_data.extend(new_data)
            
            # æ›´æ–°æ•°æ®é›†ï¼ˆè¿™é‡Œéœ€è¦å®ç°æ•°æ®èšåˆé€»è¾‘ï¼‰
            # åœ¨å®é™…å®ç°ä¸­ï¼Œéœ€è¦å°†æ–°æ•°æ®ä¿å­˜å¹¶é‡æ–°åˆ›å»ºæ•°æ®é›†
            logger.info(f"Aggregated {len(aggregated_data)} total correction samples")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        if iteration > 0 and aggregated_data:
            # ä½¿ç”¨DAggeré‡‡æ ·å™¨
            expert_indices = list(range(len(train_dataset)))
            policy_indices = list(range(len(train_dataset), len(train_dataset) + len(aggregated_data)))
            train_loader, val_loader = create_data_loaders(
                train_dataset, val_dataset, config, expert_indices, policy_indices
            )
        else:
            # ä½¿ç”¨åˆå§‹æ•°æ®
            train_loader, val_loader = create_data_loaders(train_dataset, val_dataset, config)
        
        # è®­ç»ƒå½“å‰è¿­ä»£
        logger.info(f"Training for {current_epochs} epochs...")
        
        for epoch in range(1, current_epochs + 1):
            global_epoch = iteration * current_epochs + epoch
            
            # è®­ç»ƒé˜¶æ®µ
            train_results = train_policy_epoch(
                model=policy_model,
                data_loader=train_loader,
                optimizer=optimizer,
                loss_fn=loss_fn,
                device=device,
                epoch=global_epoch,
                lr_scheduler=scheduler,
                grad_clip_norm=config['training_params'].get('grad_clip_norm', 1.0),
                logger=logger
            )
            
            # éªŒè¯é˜¶æ®µï¼ˆæ¯5ä¸ªepochä¸€æ¬¡ï¼‰
            if epoch % 5 == 0:
                val_results = evaluate_policy_epoch(
                    model=policy_model,
                    data_loader=val_loader,
                    loss_fn=loss_fn,
                    device=device,
                    epoch=global_epoch,
                    metrics=['loss', 'mse', 'action_smoothness'],
                    logger=logger
                )
                
                # è®°å½•åˆ°TensorBoard
                writer.add_scalar('Val/Loss', val_results['average_loss'], global_epoch)
                if 'mse' in val_results:
                    writer.add_scalar('Val/MSE', val_results['mse'], global_epoch)
                
                current_success_rate = 1.0 / (1.0 + val_results['average_loss'])  # ç®€åŒ–çš„æˆåŠŸç‡è®¡ç®—
            else:
                current_success_rate = 0.0
            
            # è®°å½•è®­ç»ƒæŒ‡æ ‡
            writer.add_scalar('Train/Loss', train_results['average_loss'], global_epoch)
            writer.add_scalar('Train/LearningRate', train_results['learning_rate'], global_epoch)
            writer.add_scalar('DAgger/Iteration', iteration + 1, global_epoch)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            is_best = current_success_rate > best_success_rate
            if is_best:
                best_success_rate = current_success_rate
            
            if epoch % 10 == 0 or is_best:
                save_checkpoint(
                    policy_model=policy_model,
                    optimizer=optimizer,
                    scheduler=scheduler,
                    epoch=epoch,
                    iteration=iteration,
                    best_success_rate=best_success_rate,
                    checkpoint_dir=checkpoint_dir,
                    is_best=is_best
                )
        
        logger.info(f"Iteration {iteration + 1} completed. Best success rate: {best_success_rate:.4f}")
    
    # è®­ç»ƒå®Œæˆ
    logger.info(f"DAgger training completed! Best success rate: {best_success_rate:.4f}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_checkpoint_path = os.path.join(checkpoint_dir, 'final_policy_model.pth')
    torch.save({
        'iteration': max_iterations,
        'model_state_dict': policy_model.state_dict(),
        'best_success_rate': best_success_rate,
        'config': config
    }, final_checkpoint_path)
    
    logger.info(f"Final model saved to: {final_checkpoint_path}")
    
    # æ–­å¼€æœºå™¨äººè¿æ¥
    if robot_interface:
        robot_interface.disconnect()
    
    # å…³é—­TensorBoardå†™å…¥å™¨
    writer.close()


if __name__ == '__main__':
    main()
