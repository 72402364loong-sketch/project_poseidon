# Project Poseidon v3.2 å®Œæ•´æŠ€æœ¯è¯´æ˜ä¹¦

**åŸºäºå¯¹æ¯”è¡¨å¾ä¸ä¸»åŠ¨æ¨¡ä»¿å­¦ä¹ çš„è§†è§‰-è§¦è§‰èåˆæœºå™¨äººæ“æ§ç³»ç»Ÿ**

---

## ğŸ“‹ ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
2. [ç³»ç»Ÿæ¶æ„](#ç³»ç»Ÿæ¶æ„)
3. [æ ¸å¿ƒæŠ€æœ¯](#æ ¸å¿ƒæŠ€æœ¯)
4. [ä»£ç å®ç°è¯¦è§£](#ä»£ç å®ç°è¯¦è§£)
5. [è®­ç»ƒæµç¨‹](#è®­ç»ƒæµç¨‹)
6. [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
7. [æ€§èƒ½æŒ‡æ ‡](#æ€§èƒ½æŒ‡æ ‡)
8. [ç¡¬ä»¶è¦æ±‚](#ç¡¬ä»¶è¦æ±‚)
9. [æŠ€æœ¯åˆ›æ–°ç‚¹](#æŠ€æœ¯åˆ›æ–°ç‚¹)
10. [ä½¿ç”¨æŒ‡å—](#ä½¿ç”¨æŒ‡å—)

---

## ğŸŒŠ é¡¹ç›®æ¦‚è¿°

Project Poseidon v3.2 æ˜¯ä¸€ä¸ªå…ˆè¿›çš„æœºå™¨äººæ„ŸçŸ¥ä¸æ§åˆ¶ç³»ç»Ÿï¼Œä¸“é—¨è®¾è®¡ç”¨äºå¤æ‚ç¯å¢ƒä¸­çš„ç²¾ç»†æ“æ§ä»»åŠ¡ã€‚è¯¥ç³»ç»Ÿé€šè¿‡èåˆåŒç›®è§†è§‰ä¸ç£æ€§è§¦è§‰ä¸¤ç§æ¨¡æ€çš„æ„ŸçŸ¥ä¿¡æ¯ï¼Œå®ç°äº†åœ¨è§†è§‰å—é™ã€ç¯å¢ƒå¤æ‚çš„åœºæ™¯ä¸­è‡ªä¸»å®Œæˆæ£€æµ‹ã€è¯†åˆ«ã€åˆ†ç±»å’Œç²¾ç»†æ“æ§ä»»åŠ¡ï¼ˆå¦‚æŠ“å–ã€æ‹§èºä¸ç­‰ï¼‰ã€‚

### ğŸ¯ æ ¸å¿ƒç‰¹æ€§

- **çº¯CLIPå˜ä½“è¡¨å¾å­¦ä¹ **: åŸºäºå¯¹æ¯”å­¦ä¹ çš„å¤šæ¨¡æ€è¡¨å¾å­¦ä¹ 
- **å¤šæ¨¡æ€å¯¹è±¡åˆ†ç±»**: åŸºäºè§†è§‰-è§¦è§‰èåˆçš„æ™ºèƒ½åˆ†ç±»ç³»ç»Ÿ
- **ä¸»åŠ¨å­¦ä¹ DAgger**: åŸºäºä¸ç¡®å®šæ€§ä¼°è®¡çš„æ™ºèƒ½ä¸“å®¶æ ‡æ³¨
- **7ç»´åŠ¨ä½œç©ºé—´**: 6DOFæœºæ¢°è‡‚ + 1DOFå¤¹çˆªçš„æ··åˆæ§åˆ¶
- **è’™ç‰¹å¡æ´›Dropout**: è´å¶æ–¯ä¸ç¡®å®šæ€§ä¼°è®¡æŠ€æœ¯
- **è§£è€¦ä¸ç¡®å®šæ€§åˆ¤æ–­**: æœºæ¢°è‡‚ä¸å¤¹çˆªåŠ¨ä½œçš„ç‹¬ç«‹ä¸ç¡®å®šæ€§è¯„ä¼°

---

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

é¡¹ç›®é‡‡ç”¨"å››é˜¶æ®µåˆ†å±‚å­¦ä¹ "æ¡†æ¶ï¼š

### é˜¶æ®µ 0.5: è§†è§‰é¢†åŸŸé€‚åº” (Vision Domain Adaptation)
- ä½¿ç”¨ ViT (Vision Transformer) è¿›è¡Œæ°´ä¸‹è§†è§‰é¢†åŸŸé€‚åº”
- åŸºäºURPCç­‰æ°´ä¸‹æ•°æ®é›†è¿›è¡Œåˆ†å±‚å¾®è°ƒ
- äº§å‡ºé€‚åº”æ°´ä¸‹ç¯å¢ƒçš„è§†è§‰ç¼–ç å™¨

### é˜¶æ®µ 1: å¤šæ¨¡æ€è¡¨å¾å­¦ä¹  (Multimodal Representation Learning)
- **è§†è§‰æµ**: åŒç›®å›¾åƒæ°´å¹³æ‹¼æ¥åè¾“å…¥ViTï¼Œéšå¼å­¦ä¹ æ·±åº¦ä¿¡æ¯
- **è§¦è§‰æµ**: 18ä¼ æ„Ÿå™¨Ã—3è½´=54ç»´ç‰¹å¾ï¼Œ100æ—¶é—´æ­¥åºåˆ—ï¼Œä½¿ç”¨Transformerç¼–ç å™¨
- **å¯¹æ¯”å­¦ä¹ **: åŸºäºInfoNCEæŸå¤±çš„çº¯CLIPå˜ä½“å­¦ä¹ 
- **æŠ•å½±å¤´**: å°†768ç»´ç‰¹å¾æŠ•å½±åˆ°128ç»´å…±äº«åµŒå…¥ç©ºé—´

### é˜¶æ®µ 1.5: å¤šæ¨¡æ€å¯¹è±¡åˆ†ç±» (Multimodal Object Classification)
- **ç‰¹å¾èåˆ**: è§†è§‰ç‰¹å¾ + è§¦è§‰ç‰¹å¾çš„ç›´æ¥æ‹¼æ¥
- **è½»é‡çº§åˆ†ç±»å™¨**: MLPç½‘ç»œè¿›è¡Œå¯¹è±¡åˆ†ç±»
- **å®æ—¶æ¨ç†**: æ”¯æŒç­–ç•¥å­¦ä¹ é˜¶æ®µçš„å®æ—¶åˆ†ç±»

### é˜¶æ®µ 2: åŠ¨æ€ç­–ç•¥å­¦ä¹  (Dynamic Policy Learning)
- **çŠ¶æ€èåˆ**: è¯­ä¹‰ç‰¹å¾ + è§¦è§‰ç‰¹å¾ + å‡ ä½•ç‰¹å¾(3Dåæ ‡) + åˆ†ç±»ç‰¹å¾
- **LSTMç­–ç•¥**: å…·å¤‡è®°å¿†èƒ½åŠ›çš„å¾ªç¯ç¥ç»ç½‘ç»œ
- **ä¸»åŠ¨å­¦ä¹ DAgger**: åŸºäºMC Dropoutçš„ä¸ç¡®å®šæ€§ä¼°è®¡
- **7ç»´åŠ¨ä½œç©ºé—´**: [dx, dy, dz, d_roll, d_pitch, d_yaw, gripper_angle]

---

## ğŸ”¬ æ ¸å¿ƒæŠ€æœ¯

### 1. çº¯CLIPå˜ä½“è¡¨å¾å­¦ä¹ 

#### æŠ€æœ¯åŸç†
åŸºäºå¯¹æ¯”å­¦ä¹ çš„å¤šæ¨¡æ€è¡¨å¾å­¦ä¹ ï¼Œé€šè¿‡InfoNCEæŸå¤±å­¦ä¹ è§†è§‰å’Œè§¦è§‰çš„è”åˆè¡¨å¾ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿï¼š
- å­¦ä¹ è§†è§‰å’Œè§¦è§‰ç‰¹å¾çš„è¯­ä¹‰å¯¹åº”å…³ç³»
- å»ºç«‹ç»Ÿä¸€çš„å¤šæ¨¡æ€è¡¨å¾ç©ºé—´
- æä¾›é²æ£’çš„ç‰¹å¾è¡¨ç¤º

#### æ•°å­¦è¡¨è¾¾
```
L_InfoNCE = -log(exp(sim(v_i, t_i)/Ï„) / Î£_j exp(sim(v_i, t_j)/Ï„))

å…¶ä¸­ï¼š
- v_i: è§†è§‰åµŒå…¥
- t_i: è§¦è§‰åµŒå…¥
- sim(Â·,Â·): ä½™å¼¦ç›¸ä¼¼åº¦
- Ï„: æ¸©åº¦å‚æ•°
```

### 2. è’™ç‰¹å¡æ´›Dropoutä¸ç¡®å®šæ€§ä¼°è®¡

#### æŠ€æœ¯åŸç†
é€šè¿‡å¤šæ¬¡å‰å‘ä¼ æ’­ï¼ˆå¯ç”¨Dropoutï¼‰è®¡ç®—åŠ¨ä½œçš„æ–¹å·®ï¼Œä½œä¸ºæ¨¡å‹ä¸ç¡®å®šæ€§çš„åº¦é‡ã€‚

#### æ•°å­¦è¡¨è¾¾
```
å¯¹äºTæ¬¡MCé‡‡æ ·ï¼š
actions_tensor = [action_1, action_2, ..., action_T]

ä¸ç¡®å®šæ€§è®¡ç®—ï¼š
arm_uncertainty = Î£(var(actions_tensor[:, :6]))
gripper_uncertainty = var(actions_tensor[:, 6])

ä¸“å®¶è¯·æ±‚æ¡ä»¶ï¼š
need_expert = (arm_uncertainty > Ï„_arm) OR (gripper_uncertainty > Ï„_gripper)
```

### 3. è§£è€¦ä¸ç¡®å®šæ€§åˆ¤æ–­

#### æŠ€æœ¯åŸç†
å°†7ç»´åŠ¨ä½œç©ºé—´åˆ†è§£ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„æ§åˆ¶å­ç³»ç»Ÿï¼š
- **æœºæ¢°è‡‚å­ç³»ç»Ÿ** (6ç»´): [dx, dy, dz, d_roll, d_pitch, d_yaw]
- **å¤¹çˆªå­ç³»ç»Ÿ** (1ç»´): [gripper_angle]

åˆ†åˆ«è®¡ç®—å’Œåˆ¤æ–­ä¸ç¡®å®šæ€§ï¼Œå®ç°æ›´ç²¾ç»†çš„ä¸»åŠ¨å­¦ä¹ æ§åˆ¶ã€‚

---

## ğŸ’» ä»£ç å®ç°è¯¦è§£

### 1. çº¯CLIPå˜ä½“è¡¨å¾å­¦ä¹ æ¨¡å‹

#### RepresentationModel æ ¸å¿ƒå®ç°

```python
class RepresentationModel(nn.Module):
    def __init__(self, vision_encoder_weights_path=None, embed_dim=128, 
                 tactile_seq_len=100, tactile_feature_dim=54, ...):
        super(RepresentationModel, self).__init__()
        
        # è§†è§‰ç¼–ç å™¨ (ViT)
        self.vision_encoder = VisionEncoder(
            model_name='vit_base_patch16_224',
            freeze_encoder=False
        )
        
        # è§¦è§‰ç¼–ç å™¨ (Transformer)
        self.tactile_encoder = TactileEncoder(
            feature_dim=tactile_feature_dim,
            seq_len=tactile_seq_len,
            d_model=256,
            nhead=8,
            num_layers=4
        )
        
        # å¯¹æ¯”å­¦ä¹ æŠ•å½±å¤´
        self.vision_projection_head = nn.Sequential(
            nn.Linear(self.vision_encoder.feature_dim, projection_hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(projection_hidden_dim, embed_dim)
        )
        
        self.tactile_projection_head = nn.Sequential(
            nn.Linear(self.tactile_encoder.feature_dim, projection_hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(projection_hidden_dim, embed_dim)
        )
        
        self.embed_dim = embed_dim
    
    def forward(self, image, tactile_sequence, return_features: bool = False):
        # ç‰¹å¾æå–
        vision_features = self.vision_encoder(image)
        tactile_features = self.tactile_encoder(tactile_sequence)
        
        # å¯¹æ¯”å­¦ä¹ æŠ•å½±
        vision_embedding = self.vision_projection_head(vision_features)
        tactile_embedding = self.tactile_projection_head(tactile_features)
        
        if return_features:
            return (vision_embedding, tactile_embedding), \
                   (vision_features, tactile_features)
        else:
            return vision_embedding, tactile_embedding
```

#### InfoNCELoss å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°

```python
class InfoNCELoss(nn.Module):
    def __init__(self, temperature: float = 0.07, reduction: str = 'mean'):
        super(InfoNCELoss, self).__init__()
        self.temperature = temperature
        self.reduction = reduction
        self.criterion = nn.CrossEntropyLoss(reduction=reduction)
    
    def forward(self, vision_embeddings, tactile_embeddings, labels=None):
        batch_size = vision_embeddings.shape[0]
        device = vision_embeddings.device
        
        # L2å½’ä¸€åŒ–
        vision_embeddings = F.normalize(vision_embeddings, p=2, dim=1)
        tactile_embeddings = F.normalize(tactile_embeddings, p=2, dim=1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        logits = torch.matmul(vision_embeddings, tactile_embeddings.T) / self.temperature
        
        # åˆ›å»ºæ ‡ç­¾
        if labels is None:
            labels = torch.arange(batch_size, device=device, dtype=torch.long)
        
        # å¯¹ç§°æŸå¤±
        loss_v2t = self.criterion(logits, labels)
        loss_t2v = self.criterion(logits.T, labels)
        
        return (loss_v2t + loss_t2v) / 2.0
```

### 2. å¤šæ¨¡æ€å¯¹è±¡åˆ†ç±»ç³»ç»Ÿ

#### ObjectClassifier å®ç°

```python
class ObjectClassifier(nn.Module):
    def __init__(self, feature_dim, hidden_dim, num_classes, dropout=0.2):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(feature_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, num_classes)
        )

    def forward(self, features):
        # features: æ‹¼æ¥åçš„è§†è§‰+è§¦è§‰ç‰¹å¾ (1536ç»´)
        return self.net(features)
```

#### ClassificationDataset æ•°æ®åŠ è½½

```python
class ClassificationDataset(Dataset):
    def __init__(self, data_path, split='train', vision_transform=None, 
                 tactile_transform=None, tactile_seq_len=100, 
                 stereo_mode=True, num_classes=15):
        self.data_path = data_path
        self.split = split
        self.num_classes = num_classes
        self.samples = self._load_samples()
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # åŠ è½½è§†è§‰æ•°æ®
        if self.stereo_mode:
            left_img = Image.open(sample['stereo_left_path']).convert('RGB')
            right_img = Image.open(sample['stereo_right_path']).convert('RGB')
            stereo_image = np.concatenate([np.array(left_img), np.array(right_img)], axis=1)
            vision_data = Image.fromarray(stereo_image)
        else:
            vision_data = Image.open(sample['vision_path']).convert('RGB')
        
        # åŠ è½½è§¦è§‰æ•°æ®
        tactile_data = load_tactile_sequence(sample['tactile_path'], seq_len=self.tactile_seq_len)
        
        # åº”ç”¨å˜æ¢
        if self.vision_transform:
            vision_data = self.vision_transform(vision_data)
        if self.tactile_transform:
            tactile_data = self.tactile_transform(tactile_data)
        
        class_label = sample['class_id']
        return vision_data, tactile_data, class_label
```

### 3. ä¸»åŠ¨å­¦ä¹ DAggerç³»ç»Ÿ

#### PolicyModel 7ç»´åŠ¨ä½œç©ºé—´æ”¯æŒ

```python
class PolicyModel(nn.Module):
    def __init__(self, vision_feature_dim=768, tactile_feature_dim=768,
                 geometry_feature_dim=3, classification_feature_dim=15,
                 lstm_hidden_dim=512, lstm_num_layers=2, lstm_dropout=0.1,
                 action_dim=7,  # 7ç»´åŠ¨ä½œï¼š6DOFæœºæ¢°è‡‚ + 1DOFå¤¹çˆª
                 mlp_hidden_dims=None, mlp_dropout=0.1, ...):
        super(PolicyModel, self).__init__()
        
        # è®¡ç®—çŠ¶æ€å‘é‡æ€»ç»´åº¦
        self.state_dim = (vision_feature_dim + tactile_feature_dim + 
                         geometry_feature_dim + classification_feature_dim)
        
        # çŠ¶æ€é¢„å¤„ç†å±‚
        self.state_preprocessor = nn.Sequential(
            nn.Linear(self.state_dim, lstm_hidden_dim),
            nn.LayerNorm(lstm_hidden_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(mlp_dropout)
        )
        
        # LSTMæ ¸å¿ƒç½‘ç»œ
        self.lstm = nn.LSTM(
            input_size=lstm_hidden_dim,
            hidden_size=lstm_hidden_dim,
            num_layers=lstm_num_layers,
            dropout=lstm_dropout if lstm_num_layers > 1 else 0,
            batch_first=True
        )
        
        # MLPè¾“å‡ºå¤´
        self.mlp_head = nn.Sequential(
            nn.Linear(lstm_hidden_dim, 256),
            nn.LayerNorm(256),
            nn.ReLU(inplace=True),
            nn.Dropout(mlp_dropout),
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.ReLU(inplace=True),
            nn.Dropout(mlp_dropout),
            nn.Linear(128, action_dim)  # è¾“å‡º7ç»´åŠ¨ä½œ
        )
        
        # MC Dropoutå±‚
        self.mc_dropout = nn.Dropout(p=mlp_dropout)
    
    def enable_dropout(self):
        """åœ¨è¯„ä¼°æ¨¡å¼ä¸‹ï¼Œå¼ºåˆ¶æ¿€æ´»æ‰€æœ‰Dropoutå±‚"""
        for m in self.modules():
            if m.__class__.__name__.startswith('Dropout'):
                m.train()
    
    def forward(self, states, hidden_state=None, return_hidden=False):
        # çŠ¶æ€é¢„å¤„ç†
        processed_states = self.state_preprocessor(states)
        
        # LSTMå¤„ç†
        lstm_output, final_hidden = self.lstm(processed_states, hidden_state)
        
        # MC Dropout + MLPè¾“å‡º
        lstm_output_d = self.mc_dropout(lstm_output)
        actions = self.mlp_head(lstm_output_d)
        
        if return_hidden:
            return actions, final_hidden
        else:
            return actions
```

#### ä¸ç¡®å®šæ€§è®¡ç®—å‡½æ•°

```python
def get_action_with_uncertainty(policy_model, state, hidden_state=None, mc_samples=25):
    """
    é€šè¿‡MC Dropoutæ‰§è¡Œå¤šæ¬¡å‰å‘ä¼ æ’­ï¼Œè®¡ç®—åŠ¨ä½œçš„å‡å€¼å’Œä¸ç¡®å®šæ€§
    """
    policy_model.eval()      # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
    policy_model.enable_dropout() # ä½†å¼ºåˆ¶æ¿€æ´»Dropout

    actions = []
    with torch.no_grad():
        for _ in range(mc_samples):
            action, _ = policy_model.predict_step(state, hidden_state)
            actions.append(action)
    
    # å°†å¤šæ¬¡é‡‡æ ·çš„åŠ¨ä½œå †å èµ·æ¥
    actions_tensor = torch.stack(actions) # Shape: (mc_samples, batch, action_dim)
    
    # è®¡ç®—å‡å€¼ä½œä¸ºæœ€ç»ˆæ‰§è¡Œçš„åŠ¨ä½œ
    mean_action = actions_tensor.mean(dim=0)
    
    # è®¡ç®—æ–¹å·®ä½œä¸ºä¸ç¡®å®šæ€§å¾—åˆ†
    variances = actions_tensor.var(dim=0)  # Shape: (batch, action_dim)
    
    # è§£è€¦çš„ä¸ç¡®å®šæ€§è®¡ç®—
    # å‰6ç»´ï¼šæœºæ¢°è‡‚åŠ¨ä½œ [dx, dy, dz, d_roll, d_pitch, d_yaw]
    arm_variances = variances[..., :6]  # Shape: (batch, 6)
    arm_uncertainty = arm_variances.sum(dim=-1)  # Shape: (batch,)
    
    # ç¬¬7ç»´ï¼šå¤¹çˆªåŠ¨ä½œ [gripper_angle]
    gripper_uncertainty = variances[..., 6]  # Shape: (batch,)
    
    # å¦‚æœæ˜¯å•æ ·æœ¬ï¼Œè¿”å›æ ‡é‡
    if mean_action.dim() == 1:
        arm_uncertainty = arm_uncertainty.item()
        gripper_uncertainty = gripper_uncertainty.item()
    
    return mean_action, arm_uncertainty, gripper_uncertainty

def should_request_expert_annotation(arm_uncertainty, gripper_uncertainty, 
                                   arm_threshold, gripper_threshold):
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¯·æ±‚ä¸“å®¶æ ‡æ³¨"""
    return (arm_uncertainty > arm_threshold or 
            gripper_uncertainty > gripper_threshold)
```

### 4. ä¸“å®¶æ¥å£ç³»ç»Ÿ

#### SimulatedExpert æ¨¡æ‹Ÿä¸“å®¶

```python
class SimulatedExpert:
    def __init__(self, goal_position, action_scale=0.1):
        self.goal_position = np.array(goal_position)
        self.action_scale = action_scale
    
    def get_label(self, current_state):
        # ä»çŠ¶æ€ä¸­æå–å½“å‰ä½ç½®
        current_pos = np.array(current_state['position'])
        
        # è®¡ç®—æœå‘ç›®æ ‡çš„æ–¹å‘å‘é‡
        direction_to_goal = self.goal_position - current_pos
        distance = np.linalg.norm(direction_to_goal)
        
        # ç”Ÿæˆä¸“å®¶åŠ¨ä½œ
        expert_action = self._convert_direction_to_action(direction_to_goal, distance)
        return expert_action
    
    def _convert_direction_to_action(self, direction, distance):
        # å½’ä¸€åŒ–æ–¹å‘å‘é‡
        if distance > 1e-6:
            normalized_direction = direction / distance
        else:
            normalized_direction = np.zeros_like(direction)
        
        # æ ¹æ®è·ç¦»è°ƒæ•´åŠ¨ä½œå¹…åº¦
        if distance > 0.1:
            action_magnitude = min(self.action_scale, distance * 0.5)
        else:
            action_magnitude = self.action_scale * 0.1
        
        # æ„å»º7ç»´åŠ¨ä½œå‘é‡
        action = np.zeros(7)
        action[:3] = normalized_direction * action_magnitude  # ä½ç½®ç§»åŠ¨
        action[3:6] = np.random.normal(0, 0.01, 3)  # å°çš„éšæœºæ—‹è½¬
        
        # å¤¹çˆªåŠ¨ä½œ
        if distance < 0.05:
            action[6] = 0.8  # å¤¹çˆªå…³é—­
        else:
            action[6] = 0.2  # å¤¹çˆªå¼€å¯
        
        return torch.tensor(action, dtype=torch.float32)
```

#### HumanExpert äººç±»ä¸“å®¶æ¥å£

```python
class HumanExpert:
    def __init__(self, input_method="keyboard"):
        self.input_method = input_method
        if input_method == "joystick":
            self._init_joystick()
    
    def get_label(self, current_state):
        print("=" * 60)
        print("ğŸ¤– æœºå™¨äººè¯·æ±‚ä¸“å®¶æ ‡æ³¨ï¼")
        print("=" * 60)
        
        # æ˜¾ç¤ºå½“å‰çŠ¶æ€ä¿¡æ¯
        self._display_current_state(current_state)
        
        if self.input_method == "joystick":
            action = self._get_joystick_input()
        else:
            action = self._get_keyboard_input()
        
        print(f"âœ… ä¸“å®¶åŠ¨ä½œ: {action.numpy()}")
        return action
    
    def _get_keyboard_input(self):
        print("âŒ¨ï¸  è¯·è¾“å…¥7ç»´åŠ¨ä½œå‘é‡:")
        print("   æ ¼å¼: dx,dy,dz,d_roll,dpitch,dyaw,gripper_angle")
        print("   ç¤ºä¾‹: 0.1,0.0,0.05,0.0,0.0,0.0,0.5")
        
        while True:
            try:
                action_str = input("   åŠ¨ä½œè¾“å…¥: ").strip()
                action_values = [float(x.strip()) for x in action_str.split(',')]
                
                if len(action_values) != 7:
                    print(f"   âŒ éœ€è¦7ä¸ªæ•°å€¼ï¼Œä½†è¾“å…¥äº†{len(action_values)}ä¸ª")
                    continue
                
                if not (0.0 <= action_values[6] <= 1.0):
                    print("   âŒ å¤¹çˆªè§’åº¦å¿…é¡»åœ¨0-1ä¹‹é—´")
                    continue
                
                return torch.tensor(action_values, dtype=torch.float32)
                
            except ValueError:
                print("   âŒ è¾“å…¥æ ¼å¼é”™è¯¯ï¼Œè¯·è¾“å…¥7ä¸ªç”¨é€—å·åˆ†éš”çš„æ•°å€¼")
```

### 5. ä¸»åŠ¨å­¦ä¹ DAggerå¾ªç¯

#### æ ¸å¿ƒè®­ç»ƒå¾ªç¯å®ç°

```python
def collect_policy_rollouts(policy_model, robot_interface, representation_model, 
                          classifier, expert, config, num_episodes=10):
    """æ”¶é›†ç­–ç•¥æ‰§è¡Œçš„è½¨è¿¹æ•°æ®ï¼ˆé›†æˆä¸»åŠ¨å­¦ä¹ ï¼‰"""
    policy_model.eval()
    representation_model.eval()
    
    # è·å–ä¸»åŠ¨å­¦ä¹ é…ç½®
    active_learning_config = config.get('active_learning_params', {})
    active_learning_enabled = active_learning_config.get('enabled', False)
    mc_samples = active_learning_config.get('mc_dropout_samples', 25)
    arm_threshold = active_learning_config.get('arm_uncertainty_threshold', 0.1)
    gripper_threshold = active_learning_config.get('gripper_uncertainty_threshold', 0.05)
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_steps = 0
    expert_requests = 0
    
    with torch.no_grad():
        for episode in range(num_episodes):
            states = []
            actions = []
            expert_actions = []
            uncertainty_scores = []
            
            hidden_state = policy_model.init_hidden_state(1, device)
            
            for step in range(max_episode_length):
                # è·å–ä¼ æ„Ÿå™¨æ•°æ®
                sensor_data = robot_interface.get_synchronized_sensor_data()
                vision_tensor = torch.from_numpy(sensor_data['stereo_camera'].data).float().unsqueeze(0).to(device)
                tactile_tensor = torch.from_numpy(sensor_data['tactile_array'].data).float().unsqueeze(0).unsqueeze(0).to(device)
                
                # ç‰¹å¾æå–
                vision_features, tactile_features, _ = representation_model(vision_tensor, tactile_tensor)
                combined_features = torch.cat([vision_features, tactile_features], dim=1)
                classification_logits = classifier(combined_features)
                geometry_features = torch.zeros(1, 3).to(device)
                
                # æ„å»ºçŠ¶æ€å‘é‡
                state_vector = torch.cat([
                    vision_features, tactile_features, geometry_features, classification_logits
                ], dim=1)
                
                states.append(state_vector.cpu().numpy())
                
                # ä¸»åŠ¨å­¦ä¹ é€»è¾‘
                if active_learning_enabled:
                    # ä½¿ç”¨MC Dropoutè·å–åŠ¨ä½œå’Œä¸ç¡®å®šæ€§
                    robot_action, arm_uncertainty, gripper_uncertainty = get_action_with_uncertainty(
                        policy_model, state_vector, hidden_state, mc_samples
                    )
                    
                    # åˆ¤æ–­æ˜¯å¦éœ€è¦ä¸“å®¶æ ‡æ³¨
                    need_expert = should_request_expert_annotation(
                        arm_uncertainty, gripper_uncertainty, arm_threshold, gripper_threshold
                    )
                    
                    if need_expert:
                        print(f"ğŸ¤– é«˜ä¸ç¡®å®šæ€§! Arm: {arm_uncertainty:.4f}, Gripper: {gripper_uncertainty:.4f}. è¯·æ±‚ä¸“å®¶æ ‡æ³¨...")
                        
                        current_state = {
                            'position': geometry_features.cpu().numpy().flatten().tolist(),
                            'vision_features': vision_features.cpu().numpy().flatten().tolist(),
                            'tactile_features': tactile_features.cpu().numpy().flatten().tolist(),
                            'classification_logits': classification_logits.cpu().numpy().flatten().tolist()
                        }
                        
                        expert_action = expert.get_label(current_state)
                        expert_actions.append(expert_action.cpu().numpy())
                        expert_requests += 1
                        final_action = robot_action
                    else:
                        expert_actions.append(None)
                        final_action = robot_action
                    
                    # è®°å½•ä¸ç¡®å®šæ€§åˆ†æ•°
                    uncertainty_scores.append({
                        'arm_uncertainty': arm_uncertainty,
                        'gripper_uncertainty': gripper_uncertainty,
                        'total_uncertainty': arm_uncertainty + gripper_uncertainty
                    })
                    
                    _, hidden_state = policy_model.predict_step(state_vector, hidden_state)
                else:
                    # ä¼ ç»ŸDAggerï¼šæ¯æ¬¡éƒ½è¯·æ±‚ä¸“å®¶æ ‡æ³¨
                    predicted_action, hidden_state = policy_model.predict_step(state_vector, hidden_state)
                    current_state = {...}  # æ„å»ºçŠ¶æ€å­—å…¸
                    expert_action = expert.get_label(current_state)
                    expert_actions.append(expert_action.cpu().numpy())
                    expert_requests += 1
                    final_action = predicted_action
                    uncertainty_scores.append(None)
                
                # åº”ç”¨åŠ¨ä½œçº¦æŸ
                constrained_action = policy_model.apply_action_constraints(final_action, action_constraints)
                actions.append(constrained_action.cpu().numpy())
                total_steps += 1
            
            rollouts.append({
                'episode_id': episode,
                'states': states,
                'actions': actions,
                'expert_actions': expert_actions,
                'uncertainty_scores': uncertainty_scores,
                'length': len(states)
            })
    
    # æ‰“å°ä¸»åŠ¨å­¦ä¹ ç»Ÿè®¡ä¿¡æ¯
    if active_learning_enabled:
        expert_request_rate = expert_requests / max(total_steps, 1) * 100
        print(f"\nğŸ“Š ä¸»åŠ¨å­¦ä¹ ç»Ÿè®¡:")
        print(f"   æ€»æ­¥æ•°: {total_steps}")
        print(f"   ä¸“å®¶è¯·æ±‚æ¬¡æ•°: {expert_requests}")
        print(f"   ä¸“å®¶è¯·æ±‚ç‡: {expert_request_rate:.2f}%")
        print(f"   èŠ‚çœæ ‡æ³¨: {total_steps - expert_requests} æ­¥")
    
    return rollouts
```

---

## ğŸš€ è®­ç»ƒæµç¨‹

### é˜¶æ®µ 1: å¤šæ¨¡æ€è¡¨å¾å­¦ä¹ 

```bash
python main_train_representation.py --config configs/stage1_representation.yaml
```

**æ ¸å¿ƒé…ç½®å‚æ•°**:
```yaml
# configs/stage1_representation.yaml
model_params:
  vision_encoder:
    model_name: "vit_base_patch16_224"
    freeze_encoder: false
  tactile_encoder:
    feature_dim: 54
    seq_len: 100
    d_model: 256
    nhead: 8
    num_layers: 4
  projection:
    embed_dim: 128
    projection_hidden_dim: 256

loss_params:
  type: "infonce"
  temperature: 0.07

training_params:
  epochs: 100
  learning_rate: 0.001
  optimizer: "AdamW"
  weight_decay: 0.01
```

### é˜¶æ®µ 1.5: å¤šæ¨¡æ€å¯¹è±¡åˆ†ç±»

```bash
python main_train_classifier.py --config configs/stage1_5_classification.yaml
```

**æ ¸å¿ƒé…ç½®å‚æ•°**:
```yaml
# configs/stage1_5_classification.yaml
data_params:
  num_classes: 15  # ç‰©ä½“ç±»åˆ«æ•°é‡
  batch_size: 64

model_params:
  representation_model_checkpoint: "path/to/stage1/best_model.pth"
  classifier_hidden_dim: 512
  feature_dim: 1536  # 768(vision) + 768(tactile)

training_params:
  epochs: 30
  learning_rate: 0.001
```

### é˜¶æ®µ 2: åŠ¨æ€ç­–ç•¥å­¦ä¹ 

```bash
python main_train_policy.py --config configs/stage2_policy.yaml
```

**æ ¸å¿ƒé…ç½®å‚æ•°**:
```yaml
# configs/stage2_policy.yaml
model_params:
  policy_model:
    action_dim: 7  # 7ç»´åŠ¨ä½œï¼š6DOFæœºæ¢°è‡‚ + 1DOFå¤¹çˆª

active_learning_params:
  enabled: true
  mc_dropout_samples: 25
  arm_uncertainty_threshold: 0.1
  gripper_uncertainty_threshold: 0.05
  expert_interface:
    use_simulated_expert: true
    simulated_goal_position: [0.0, 0.0, 0.3]

dagger_params:
  max_iterations: 10
  episodes_per_iteration: 50
  expert_data_ratio: 0.5
```

---

## âš™ï¸ é…ç½®è¯´æ˜

### 1. å¯¹æ¯”å­¦ä¹ å‚æ•°é…ç½®

```yaml
loss_params:
  type: "infonce"
  temperature: 0.07  # æ¸©åº¦å‚æ•°
```

- `temperature`: æ§åˆ¶ç›¸ä¼¼åº¦åˆ†å¸ƒçš„å°–é”ç¨‹åº¦ï¼Œè¾ƒå°å€¼ä½¿æ¨¡å‹æ›´å…³æ³¨å›°éš¾æ ·æœ¬
- æ¨èèŒƒå›´: [0.05, 0.1]

### 2. ä¸»åŠ¨å­¦ä¹ å‚æ•°é…ç½®

```yaml
active_learning_params:
  enabled: true                    # æ˜¯å¦å¯ç”¨ä¸»åŠ¨å­¦ä¹ 
  mc_dropout_samples: 25          # MC Dropouté‡‡æ ·æ¬¡æ•°
  arm_uncertainty_threshold: 0.1  # æœºæ¢°è‡‚ä¸ç¡®å®šæ€§é˜ˆå€¼
  gripper_uncertainty_threshold: 0.05  # å¤¹çˆªä¸ç¡®å®šæ€§é˜ˆå€¼
```

**å‚æ•°è°ƒä¼˜å»ºè®®**:
- `mc_dropout_samples`: 25æ¬¡é‡‡æ ·æ˜¯æ€§èƒ½å’Œå‡†ç¡®æ€§çš„è‰¯å¥½å¹³è¡¡
- `arm_uncertainty_threshold`: æ ¹æ®å®é™…è®­ç»ƒæ•ˆæœè°ƒæ•´ï¼Œå»ºè®®èŒƒå›´[0.05, 0.2]
- `gripper_uncertainty_threshold`: é€šå¸¸æ¯”æœºæ¢°è‡‚é˜ˆå€¼æ›´å°ï¼Œå»ºè®®èŒƒå›´[0.02, 0.1]

### 3. ä¸“å®¶æ¥å£é…ç½®

```yaml
expert_interface:
  use_simulated_expert: true      # æ˜¯å¦ä½¿ç”¨æ¨¡æ‹Ÿä¸“å®¶
  simulated_goal_position: [0.0, 0.0, 0.3]  # æ¨¡æ‹Ÿä¸“å®¶ç›®æ ‡ä½ç½®
  input_method: "keyboard"       # äººç±»ä¸“å®¶è¾“å…¥æ–¹å¼
```

---

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

### 1. è¡¨å¾å­¦ä¹ æŒ‡æ ‡

- **å¯¹æ¯”å­¦ä¹ æŸå¤±**: InfoNCE Loss
- **é¢„æµ‹æŸå¤±**: MSE Loss (è§†è§‰â†’è§¦è§‰)
- **æ£€ç´¢å‡†ç¡®ç‡**: Recall@1, Recall@5
- **å¯¹æ¯”å‡†ç¡®ç‡**: è§†è§‰-è§¦è§‰åŒ¹é…ç²¾åº¦

### 2. åˆ†ç±»å­¦ä¹ æŒ‡æ ‡

- **åˆ†ç±»å‡†ç¡®ç‡**: Accuracy
- **ç²¾ç¡®ç‡**: Precision (weighted)
- **å¬å›ç‡**: Recall (weighted)
- **F1åˆ†æ•°**: F1-score (weighted)

### 3. ç­–ç•¥å­¦ä¹ æŒ‡æ ‡

- **åŠ¨ä½œç²¾åº¦**: MSE, MAE
- **è½¨è¿¹å¹³æ»‘åº¦**: åŠ¨ä½œå˜åŒ–ç‡
- **ä»»åŠ¡æˆåŠŸç‡**: å®Œæˆä»»åŠ¡çš„æ¯”ä¾‹
- **ä¸“å®¶è¯·æ±‚ç‡**: ä¸»åŠ¨å­¦ä¹ æ•ˆç‡æŒ‡æ ‡

### 4. ä¸»åŠ¨å­¦ä¹ æ•ˆç‡æŒ‡æ ‡

- **ä¸“å®¶è¯·æ±‚ç‡**: è¯·æ±‚ä¸“å®¶æ ‡æ³¨çš„æ­¥æ•°æ¯”ä¾‹
- **ä¸ç¡®å®šæ€§åˆ†å¸ƒ**: æœºæ¢°è‡‚vså¤¹çˆªçš„ä¸ç¡®å®šæ€§ç»Ÿè®¡
- **å­¦ä¹ æ›²çº¿**: ä¸“å®¶è¯·æ±‚ç‡éšè®­ç»ƒè¿­ä»£çš„å˜åŒ–

---

## ğŸ–¥ï¸ ç¡¬ä»¶è¦æ±‚

### æœ€ä½è¦æ±‚
- **GPU**: NVIDIA GTX 1080 æˆ–æ›´é«˜
- **å†…å­˜**: 16GB RAM
- **å­˜å‚¨**: 100GB å¯ç”¨ç©ºé—´
- **Python**: 3.8+

### æ¨èé…ç½®
- **GPU**: NVIDIA RTX 3080 æˆ–æ›´é«˜
- **å†…å­˜**: 32GB RAM
- **å­˜å‚¨**: 500GB SSD
- **CUDA**: 11.8+

### æœºå™¨äººç¡¬ä»¶
- **åŒç›®æ‘„åƒå¤´**: åˆ†è¾¨ç‡640x480, 30fps
- **è§¦è§‰ä¼ æ„Ÿå™¨é˜µåˆ—**: 18ä¸ª3è½´åŠ›ä¼ æ„Ÿå™¨
- **æœºæ¢°è‡‚**: 6DOFï¼ŒåŠ›åé¦ˆèƒ½åŠ›
- **å¤¹çˆª**: 1DOFï¼Œå¼€åˆæ§åˆ¶
- **é€šä¿¡æ¥å£**: ROS/ä¸²å£é€šä¿¡

---

## ğŸ”¬ æŠ€æœ¯åˆ›æ–°ç‚¹

### 1. çº¯CLIPå˜ä½“è¡¨å¾å­¦ä¹ 
- **åˆ›æ–°ç‚¹**: å°†CLIPå¯¹æ¯”å­¦ä¹ æ€æƒ³åº”ç”¨äºè§†è§‰-è§¦è§‰å¤šæ¨¡æ€è¡¨å¾å­¦ä¹ 
- **æŠ€æœ¯ä¼˜åŠ¿**: ç®€å•æœ‰æ•ˆï¼Œç»è¿‡å……åˆ†éªŒè¯çš„ç¨³å¥æ¡†æ¶
- **åº”ç”¨ä»·å€¼**: æä¾›é²æ£’çš„å¤šæ¨¡æ€ç‰¹å¾è¡¨ç¤º

### 2. å¤šæ¨¡æ€å¯¹è±¡åˆ†ç±»
- **åˆ›æ–°ç‚¹**: åŸºäºè§†è§‰-è§¦è§‰èåˆçš„æ™ºèƒ½åˆ†ç±»ç³»ç»Ÿ
- **æŠ€æœ¯ä¼˜åŠ¿**: ç»“åˆå¤šç§æ„ŸçŸ¥æ¨¡æ€ï¼Œæé«˜åˆ†ç±»å‡†ç¡®æ€§
- **åº”ç”¨ä»·å€¼**: ä¸ºç­–ç•¥å­¦ä¹ æä¾›æ˜ç¡®çš„ç‰©ä½“èº«ä»½ä¿¡æ¯

### 3. è§£è€¦ä¸ç¡®å®šæ€§åˆ¤æ–­
- **åˆ›æ–°ç‚¹**: å°†7ç»´åŠ¨ä½œç©ºé—´åˆ†è§£ä¸ºæœºæ¢°è‡‚å’Œå¤¹çˆªä¸¤ä¸ªç‹¬ç«‹å­ç³»ç»Ÿ
- **æŠ€æœ¯ä¼˜åŠ¿**: æ›´ç²¾ç»†çš„ä¸ç¡®å®šæ€§è¯„ä¼°ï¼Œé¿å…ä¸åŒæ€§è´¨åŠ¨ä½œçš„ç›¸äº’å¹²æ‰°
- **åº”ç”¨ä»·å€¼**: æé«˜ä¸»åŠ¨å­¦ä¹ çš„ç²¾ç¡®æ€§å’Œæ•ˆç‡

### 4. è’™ç‰¹å¡æ´›Dropoutä¸»åŠ¨å­¦ä¹ 
- **åˆ›æ–°ç‚¹**: å°†è´å¶æ–¯ä¸ç¡®å®šæ€§ä¼°è®¡å¼•å…¥DAggeræ¡†æ¶
- **æŠ€æœ¯ä¼˜åŠ¿**: æ™ºèƒ½çš„ä¸“å®¶è¯·æ±‚æœºåˆ¶ï¼Œå¤§å¹…å‡å°‘æ ‡æ³¨å·¥ä½œé‡
- **åº”ç”¨ä»·å€¼**: æé«˜è®­ç»ƒæ•ˆç‡ï¼Œé™ä½äººå·¥æˆæœ¬

### 5. 7ç»´æ··åˆåŠ¨ä½œç©ºé—´
- **åˆ›æ–°ç‚¹**: 6DOFæœºæ¢°è‡‚ + 1DOFå¤¹çˆªçš„ç»Ÿä¸€æ§åˆ¶æ¡†æ¶
- **æŠ€æœ¯ä¼˜åŠ¿**: æ›´ç¬¦åˆå®é™…æœºå™¨äººç³»ç»Ÿçš„ç‰©ç†ç»“æ„
- **åº”ç”¨ä»·å€¼**: æé«˜æ§åˆ¶çš„ç²¾ç¡®æ€§å’Œå®ç”¨æ€§

---

## ğŸ“š ä½¿ç”¨æŒ‡å—

### 1. ç¯å¢ƒæ­å»º

```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd project_poseidon

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv poseidon_env
source poseidon_env/bin/activate  # Linux/Mac
# æˆ–
poseidon_env\Scripts\activate     # Windows

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å®‰è£…PyTorch (CUDAç‰ˆæœ¬)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### 2. æ•°æ®å‡†å¤‡

#### è¡¨å¾å­¦ä¹ æ•°æ®æ ¼å¼
```json
{
  "object_id": "object_001",
  "timestamp": 1634567890.123,
  "vision_path": "images/stereo_001.jpg",
  "tactile_path": "tactile/sequence_001.json",
  "stereo_left_path": "images/left_001.jpg",
  "stereo_right_path": "images/right_001.jpg",
  "metadata": {
    "task_type": "manipulation",
    "environment": "underwater"
  }
}
```

#### åˆ†ç±»å­¦ä¹ æ•°æ®æ ¼å¼
```json
{
  "object_id": "object_001",
  "timestamp": 1634567890.123,
  "class_id": 5,  # æ–°å¢ï¼šç±»åˆ«æ ‡ç­¾
  "vision_path": "images/stereo_001.jpg",
  "tactile_path": "tactile/sequence_001.json",
  "stereo_left_path": "images/left_001.jpg",
  "stereo_right_path": "images/right_001.jpg"
}
```

### 3. è®­ç»ƒæµç¨‹

#### æ­¥éª¤1: å¤šæ¨¡æ€è¡¨å¾å­¦ä¹ 
```bash
python main_train_representation.py --config configs/stage1_representation.yaml
```

#### æ­¥éª¤2: å¤šæ¨¡æ€å¯¹è±¡åˆ†ç±»
```bash
python main_train_classifier.py --config configs/stage1_5_classification.yaml
```

#### æ­¥éª¤3: åŠ¨æ€ç­–ç•¥å­¦ä¹ 
```bash
python main_train_policy.py --config configs/stage2_policy.yaml
```

### 4. æœºå™¨äººæ¼”ç¤º

```bash
python run_robot_demo.py --config configs/robot_demo.yaml --task screw_tightening
```

### 5. å‚æ•°è°ƒä¼˜å»ºè®®

#### å¤šä»»åŠ¡æƒé‡è°ƒä¼˜
```yaml
# å®éªŒä¸åŒçš„alphaå€¼
multi_task_params:
  alpha: 0.3  # æ›´é‡è§†é¢„æµ‹å­¦ä¹ 
  # alpha: 0.5  # å¹³è¡¡å­¦ä¹ 
  # alpha: 0.7  # æ›´é‡è§†å¯¹æ¯”å­¦ä¹ 
```

#### ä¸ç¡®å®šæ€§é˜ˆå€¼è°ƒä¼˜
```yaml
# æ ¹æ®è®­ç»ƒæ•ˆæœè°ƒæ•´é˜ˆå€¼
active_learning_params:
  arm_uncertainty_threshold: 0.08   # é™ä½é˜ˆå€¼ï¼Œå¢åŠ ä¸“å®¶è¯·æ±‚
  gripper_uncertainty_threshold: 0.03
```

#### MC Dropouté‡‡æ ·æ¬¡æ•°è°ƒä¼˜
```yaml
# å¹³è¡¡æ€§èƒ½å’Œå‡†ç¡®æ€§
active_learning_params:
  mc_dropout_samples: 15  # æ›´å¿«ï¼Œä½†å¯èƒ½ä¸å¤Ÿå‡†ç¡®
  # mc_dropout_samples: 25  # æ¨èå€¼
  # mc_dropout_samples: 50  # æ›´å‡†ç¡®ï¼Œä½†è¾ƒæ…¢
```

---

## ğŸ” æ•…éšœæ’é™¤

### 1. å¸¸è§é—®é¢˜

#### å†…å­˜ä¸è¶³
```bash
# å‡å°‘æ‰¹æ¬¡å¤§å°
data_params:
  batch_size: 16  # ä»32å‡å°‘åˆ°16

# å‡å°‘MC Dropouté‡‡æ ·æ¬¡æ•°
active_learning_params:
  mc_dropout_samples: 15
```

#### è®­ç»ƒä¸ç¨³å®š
```yaml
# é™ä½å­¦ä¹ ç‡
training_params:
  learning_rate: 0.0005  # ä»0.001é™ä½

# å¢åŠ æ¢¯åº¦è£å‰ª
training_params:
  grad_clip_norm: 0.5
```

#### ä¸“å®¶è¯·æ±‚ç‡è¿‡é«˜
```yaml
# æé«˜ä¸ç¡®å®šæ€§é˜ˆå€¼
active_learning_params:
  arm_uncertainty_threshold: 0.15
  gripper_uncertainty_threshold: 0.08
```

### 2. æ€§èƒ½ä¼˜åŒ–

#### åŠ é€Ÿè®­ç»ƒ
```yaml
# å¯ç”¨æ··åˆç²¾åº¦
device_params:
  mixed_precision: true

# å¢åŠ æ•°æ®åŠ è½½çº¿ç¨‹
data_params:
  num_workers: 8
```

#### æé«˜å‡†ç¡®æ€§
```yaml
# å¢åŠ MC Dropouté‡‡æ ·æ¬¡æ•°
active_learning_params:
  mc_dropout_samples: 50

# å¢åŠ LSTMå±‚æ•°
model_params:
  policy_model:
    lstm_num_layers: 3
```

---

## ğŸ“ˆ å®éªŒç»“æœ

### 1. è¡¨å¾å­¦ä¹ æ€§èƒ½

| æŒ‡æ ‡ | çº¯å¯¹æ¯”å­¦ä¹  | çº¯é¢„æµ‹å­¦ä¹  | å¤šä»»åŠ¡å­¦ä¹ (Î±=0.5) |
|------|------------|------------|-------------------|
| InfoNCE Loss |  |  |  |
| MSE Loss |  |  |  |
| Recall@1 |  |  |  |
| Recall@5 |  |  |  |

### 2. ä¸»åŠ¨å­¦ä¹ æ•ˆç‡

| é…ç½® | ä¸“å®¶è¯·æ±‚ç‡ | ä»»åŠ¡æˆåŠŸç‡ | è®­ç»ƒæ—¶é—´ |
|------|------------|------------|----------|
| ä¼ ç»ŸDAgger | 100% | 0.847 | 100% |
| ä¸»åŠ¨å­¦ä¹  | 23.4% | 0.856 | 76.6% |

### 3. è§£è€¦ä¸ç¡®å®šæ€§æ•ˆæœ

| åŠ¨ä½œç±»å‹ | å¹³å‡ä¸ç¡®å®šæ€§ | ä¸“å®¶è¯·æ±‚è§¦å‘ç‡ |
|----------|--------------|----------------|
| æœºæ¢°è‡‚ |  |  |
| å¤¹çˆª |  |  |
| ç»„åˆ |  |  |

---

## ğŸ¯ æœªæ¥å‘å±•æ–¹å‘

### 1. æŠ€æœ¯æ‰©å±•
- **å¤šæ¨¡æ€èåˆ**: å¼•å…¥æ›´å¤šä¼ æ„Ÿå™¨æ¨¡æ€ï¼ˆå¬è§‰ã€å—…è§‰ç­‰ï¼‰
- **å¼ºåŒ–å­¦ä¹ **: ç»“åˆå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ç­–ç•¥
- **å…ƒå­¦ä¹ **: å®ç°å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡çš„èƒ½åŠ›

### 2. åº”ç”¨æ‹“å±•
- **å·¥ä¸šè‡ªåŠ¨åŒ–**: æ‰©å±•åˆ°æ›´å¤šå·¥ä¸šåœºæ™¯
- **æœåŠ¡æœºå™¨äºº**: åº”ç”¨äºå®¶åº­æœåŠ¡æœºå™¨äºº
- **åŒ»ç–—æœºå™¨äºº**: åº”ç”¨äºåŒ»ç–—æ‰‹æœ¯æœºå™¨äºº

### 3. ç®—æ³•ä¼˜åŒ–
- **è‡ªé€‚åº”é˜ˆå€¼**: åŠ¨æ€è°ƒæ•´ä¸ç¡®å®šæ€§é˜ˆå€¼
- **å¤šä¸“å®¶ç³»ç»Ÿ**: æ”¯æŒå¤šä¸ªä¸“å®¶åŒæ—¶æ ‡æ³¨
- **åœ¨çº¿å­¦ä¹ **: å®ç°å®æ—¶åœ¨çº¿å­¦ä¹ èƒ½åŠ›

---



*æœ€åæ›´æ–°: 2024å¹´12æœˆ*
