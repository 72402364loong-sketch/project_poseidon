"""
Vision Encoder for Project Poseidon
åŸºäºViTçš„è§†è§‰ç¼–ç å™¨ï¼Œç”¨äºæå–å›¾åƒç‰¹å¾
"""

import torch
import torch.nn as nn
import timm
from typing import Optional


class ViTEncoder(nn.Module):
    """
    Vision Transformerç¼–ç å™¨
    å°è£…é¢„è®­ç»ƒçš„ViTæ¨¡å‹ï¼Œç”¨äºä»è¾“å…¥å›¾åƒä¸­æå–é«˜è´¨é‡çš„ç‰¹å¾å‘é‡
    """
    
    def __init__(
        self,
        model_name: str = 'vit_base_patch16_224',
        pretrained: bool = True,
        num_classes: Optional[int] = None,
        freeze_layers: int = 0
    ):
        """
        Args:
            model_name: ViTæ¨¡å‹åç§°
            pretrained: æ˜¯å¦ä½¿ç”¨ImageNeté¢„è®­ç»ƒæƒé‡
            num_classes: åˆ†ç±»å¤´çš„ç±»åˆ«æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ç§»é™¤åˆ†ç±»å¤´
            freeze_layers: å†»ç»“çš„å±‚æ•°ï¼ˆä»åº•å±‚å¼€å§‹ï¼‰
        """
        super(ViTEncoder, self).__init__()
        
        self.model_name = model_name
        self.pretrained = pretrained
        self.num_classes = num_classes
        self.freeze_layers = freeze_layers
        
        # åŠ è½½é¢„è®­ç»ƒçš„ViTæ¨¡å‹
        try:
            self.vit = timm.create_model(
                model_name,
                pretrained=pretrained,
                num_classes=num_classes if num_classes is not None else 0
            )
        except Exception as e:
            print(f"âš ï¸  ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æœ¬åœ°ç¼“å­˜æˆ–ç¦»çº¿æ¨¡å¼: {e}")
            # å°è¯•ä¸ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
            if pretrained:
                print("ğŸ”„ å°è¯•ä¸ä½¿ç”¨é¢„è®­ç»ƒæƒé‡åŠ è½½æ¨¡å‹...")
                self.vit = timm.create_model(
                    model_name,
                    pretrained=False,
                    num_classes=num_classes if num_classes is not None else 0
                )
                print("âš ï¸  è­¦å‘Šï¼šæ¨¡å‹å·²åŠ è½½ä½†æœªä½¿ç”¨é¢„è®­ç»ƒæƒé‡ï¼Œæ€§èƒ½å¯èƒ½å—å½±å“")
            else:
                raise e
        
        # å¦‚æœä¸éœ€è¦åˆ†ç±»ï¼Œå°†headæ›¿æ¢ä¸ºIdentity
        if num_classes is None:
            self.vit.head = nn.Identity()
        
        # å†»ç»“æŒ‡å®šå±‚æ•°çš„æƒé‡
        if freeze_layers > 0:
            self._freeze_layers(freeze_layers)
        
        # è·å–ç‰¹å¾ç»´åº¦
        self.feature_dim = self._get_feature_dim()
    
    def _freeze_layers(self, num_layers: int) -> None:
        """
        å†»ç»“æŒ‡å®šæ•°é‡çš„Transformerå±‚
        
        Args:
            num_layers: è¦å†»ç»“çš„å±‚æ•°
        """
        # å†»ç»“patch embeddingå’Œä½ç½®ç¼–ç 
        if hasattr(self.vit, 'patch_embed'):
            for param in self.vit.patch_embed.parameters():
                param.requires_grad = False
        
        if hasattr(self.vit, 'pos_embed'):
            self.vit.pos_embed.requires_grad = False
        
        if hasattr(self.vit, 'cls_token'):
            self.vit.cls_token.requires_grad = False
        
        # å†»ç»“æŒ‡å®šæ•°é‡çš„Transformerå—
        if hasattr(self.vit, 'blocks'):
            for i, block in enumerate(self.vit.blocks):
                if i < num_layers:
                    for param in block.parameters():
                        param.requires_grad = False
        elif hasattr(self.vit, 'layers'):
            # æŸäº›ViTå®ç°ä½¿ç”¨'layers'è€Œä¸æ˜¯'blocks'
            for i, layer in enumerate(self.vit.layers):
                if i < num_layers:
                    for param in layer.parameters():
                        param.requires_grad = False
    
    def _get_feature_dim(self) -> int:
        """è·å–ç‰¹å¾ç»´åº¦"""
        # åˆ›å»ºä¸€ä¸ªdummyè¾“å…¥æ¥è·å–è¾“å‡ºç»´åº¦
        dummy_input = torch.randn(1, 3, 224, 224)
        with torch.no_grad():
            dummy_output = self.vit(dummy_input)
            if isinstance(dummy_output, tuple):
                feature_dim = dummy_output[0].shape[-1]
            else:
                feature_dim = dummy_output.shape[-1]
        
        return feature_dim
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥å›¾åƒå¼ é‡ï¼Œå½¢çŠ¶ä¸º (N, 3, H, W)
        
        Returns:
            ç‰¹å¾å‘é‡ï¼Œå½¢çŠ¶ä¸º (N, feature_dim)
        """
        # ç¡®ä¿è¾“å…¥å°ºå¯¸æ­£ç¡®
        if x.shape[-2:] != (224, 224):
            x = torch.nn.functional.interpolate(
                x, size=(224, 224), mode='bilinear', align_corners=False
            )
        
        # é€šè¿‡ViTæ¨¡å‹
        features = self.vit(x)
        
        # å¦‚æœè¾“å‡ºæ˜¯å…ƒç»„ï¼ˆæŸäº›ViTå®ç°ï¼‰ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
        if isinstance(features, tuple):
            features = features[0]
        
        return features
    
    def get_attention_maps(self, x: torch.Tensor, layer_idx: int = -1) -> torch.Tensor:
        """
        è·å–æ³¨æ„åŠ›å›¾ï¼ˆå¯é€‰åŠŸèƒ½ï¼Œç”¨äºå¯è§†åŒ–ï¼‰
        
        Args:
            x: è¾“å…¥å›¾åƒå¼ é‡
            layer_idx: è¦æå–æ³¨æ„åŠ›çš„å±‚ç´¢å¼•ï¼Œ-1è¡¨ç¤ºæœ€åä¸€å±‚
        
        Returns:
            æ³¨æ„åŠ›å›¾å¼ é‡
        """
        # è¿™ä¸ªåŠŸèƒ½éœ€è¦ä¿®æ”¹ViTçš„forwardæ–¹æ³•æ¥è¿”å›æ³¨æ„åŠ›æƒé‡
        # è¿™é‡Œæä¾›ä¸€ä¸ªåŸºç¡€å®ç°æ¡†æ¶
        
        def hook_fn(module, input, output):
            # å­˜å‚¨æ³¨æ„åŠ›æƒé‡
            if hasattr(output, 'attn_weights'):
                return output.attn_weights
            return None
        
        # æ³¨å†Œhook
        target_layer = None
        if hasattr(self.vit, 'blocks'):
            if layer_idx == -1:
                target_layer = self.vit.blocks[-1]
            else:
                target_layer = self.vit.blocks[layer_idx]
        
        if target_layer is not None:
            handle = target_layer.register_forward_hook(hook_fn)
            
            # å‰å‘ä¼ æ’­
            with torch.no_grad():
                _ = self.forward(x)
            
            # ç§»é™¤hook
            handle.remove()
        
        # æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å®ç°ï¼Œå®é™…çš„æ³¨æ„åŠ›æå–å¯èƒ½éœ€è¦æ›´å¤æ‚çš„å¤„ç†
        return None
    
    def unfreeze_layers(self, num_layers: int = None) -> None:
        """
        è§£å†»æŒ‡å®šæ•°é‡çš„å±‚
        
        Args:
            num_layers: è¦è§£å†»çš„å±‚æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™è§£å†»æ‰€æœ‰å±‚
        """
        if num_layers is None:
            # è§£å†»æ‰€æœ‰å‚æ•°
            for param in self.vit.parameters():
                param.requires_grad = True
        else:
            # è§£å†»æŒ‡å®šæ•°é‡çš„å±‚ï¼ˆä»é¡¶å±‚å¼€å§‹ï¼‰
            if hasattr(self.vit, 'blocks'):
                total_layers = len(self.vit.blocks)
                start_layer = max(0, total_layers - num_layers)
                
                for i in range(start_layer, total_layers):
                    for param in self.vit.blocks[i].parameters():
                        param.requires_grad = True
            
            # è§£å†»åˆ†ç±»å¤´ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if hasattr(self.vit, 'head') and not isinstance(self.vit.head, nn.Identity):
                for param in self.vit.head.parameters():
                    param.requires_grad = True
    
    def get_trainable_parameters(self) -> int:
        """è·å–å¯è®­ç»ƒå‚æ•°æ•°é‡"""
        return sum(p.numel() for p in self.vit.parameters() if p.requires_grad)
    
    def get_total_parameters(self) -> int:
        """è·å–æ€»å‚æ•°æ•°é‡"""
        return sum(p.numel() for p in self.vit.parameters())
    
    def print_model_info(self) -> None:
        """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
        trainable_params = self.get_trainable_parameters()
        total_params = self.get_total_parameters()
        
        print(f"ViT Encoder Model Info:")
        print(f"  Model Name: {self.model_name}")
        print(f"  Pretrained: {self.pretrained}")
        print(f"  Feature Dimension: {self.feature_dim}")
        print(f"  Total Parameters: {total_params:,}")
        print(f"  Trainable Parameters: {trainable_params:,}")
        print(f"  Frozen Parameters: {total_params - trainable_params:,}")
        print(f"  Trainable Ratio: {trainable_params / total_params * 100:.2f}%")
